{"cells":[{"cell_type":"markdown","metadata":{"id":"tJnX4apVcupg"},"source":["## Imports and drive mount"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":118147,"status":"ok","timestamp":1736018186736,"user":{"displayName":"Iker García Barranco","userId":"18047326103332880325"},"user_tz":-60},"id":"_KqfD_wLwUDe","outputId":"b284ba9c-0bb0-40b9-ad32-41a4c4f32070"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n","  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n","Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2024.10.0\n","    Uninstalling fsspec-2024.10.0:\n","      Successfully uninstalled fsspec-2024.10.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n","Collecting torchinfo\n","  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n","Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.8.0\n","Collecting fasttext\n","  Downloading fasttext-0.9.3.tar.gz (73 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting pybind11>=2.2 (from fasttext)\n","  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n","Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (75.1.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.26.4)\n","Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n","Building wheels for collected packages: fasttext\n","  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fasttext: filename=fasttext-0.9.3-cp310-cp310-linux_x86_64.whl size=4296183 sha256=7d0ecb50222170e423099f7f804f125d8146e7fbf4a6f01cec8ac19ced245591\n","  Stored in directory: /root/.cache/pip/wheels/0d/a2/00/81db54d3e6a8199b829d58e02cec2ddb20ce3e59fad8d3c92a\n","Successfully built fasttext\n","Installing collected packages: pybind11, fasttext\n","Successfully installed fasttext-0.9.3 pybind11-2.13.6\n"]}],"source":["!pip install datasets\n","!pip install torchinfo\n","!pip install fasttext"]},{"cell_type":"markdown","source":["Creamos la carpeta model_storage"],"metadata":{"id":"f7oKfX41LMUR"}},{"cell_type":"code","source":["!mkdir model_storage"],"metadata":{"id":"T9KH9qxVLLJJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tPsWThLeZcPX"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import json\n","from collections import Counter\n","import string\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.tree import DecisionTreeClassifier\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","from torchinfo import summary\n","\n","from transformers import AutoTokenizer, AutoModelForMaskedLM\n","\n","from datasets import DatasetDict, load_from_disk\n","\n","from argparse import Namespace\n","import fasttext.util"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26649,"status":"ok","timestamp":1736018245328,"user":{"displayName":"Iker García Barranco","userId":"18047326103332880325"},"user_tz":-60},"id":"EewSUx2h0HNc","outputId":"c27e6617-3697-4572-c78c-2df5aa11161e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"CxmdAfIDdqlx"},"source":["## TF - IDF"]},{"cell_type":"markdown","metadata":{"id":"JqnHJWn0a2vW"},"source":["Path para cargar los datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XLO1BpBniXcr"},"outputs":[],"source":["# Path selection\n","# file_path_gl = '/content/drive/My Drive/4º Carrera/NLP/Proyecto/gl_dataset.csv'  # PATH: Luis\n","file_path_gl = '/content/drive/MyDrive/4.Curso/NLP/Proyecto/gl_dataset.csv'  # PATH: Iker\n","\n","# file_path_pt = '/content/drive/My Drive/4º Carrera/NLP/Proyecto/pt_dataset.csv'  # PATH: Luis\n","file_path_pt = '/content/drive/MyDrive/4.Curso/NLP/Proyecto/pt_dataset.csv'  # PATH: Iker"]},{"cell_type":"markdown","metadata":{"id":"xXYpkSR5B2-O"},"source":["Utilizando ambos datasets al completo"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51880,"status":"ok","timestamp":1732362442864,"user":{"displayName":"Iker García Barranco","userId":"18047326103332880325"},"user_tz":-60},"id":"Jrb2TRlTBaDJ","outputId":"ca338615-e6f1-4712-caba-9ac1c89563ee"},"outputs":[{"name":"stdout","output_type":"stream","text":["Portugués: 552025 filas seleccionadas\n","Gallego: 598375 filas seleccionadas\n","Total combinado: 1150400 filas\n"]}],"source":["# Cargar ambos datasets\n","df_pt = pd.read_csv(file_path_pt)\n","df_gl = pd.read_csv(file_path_gl)\n","\n","# Combinar los datasets\n","df_combined = pd.concat([df_pt, df_gl], ignore_index=True)\n","\n","# Verificar resultados\n","print(f\"Portugués: {len(df_pt)} filas seleccionadas\")\n","print(f\"Gallego: {len(df_gl)} filas seleccionadas\")\n","print(f\"Total combinado: {len(df_combined)} filas\")"]},{"cell_type":"markdown","metadata":{"id":"h5mwCn6m2_fY"},"source":["Si ya tenemos el dataset completo en un csv"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wzQTp5Nn1zEC"},"outputs":[],"source":["# df_combined = pd.read_csv('df_combined.csv', ignore_index=True)"]},{"cell_type":"markdown","metadata":{"id":"Z8_69KNb3OzB"},"source":["TF-IDF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-8SplDFd-0Xg"},"outputs":[],"source":["# Prepara el train y test sets para trabajar con TfidfVectorizer\n","X_train = df_combined[df_combined['split']=='train'][\"text\"]\n","y_train = df_combined[df_combined['split']=='train']['language']\n","X_test = df_combined[df_combined['split']=='test'][\"text\"]\n","y_test = df_combined[df_combined['split']=='test']['language']\n","\n","# Aplicamos TF-IDF\n","vectorizer = TfidfVectorizer(stop_words='english')\n","X_train_tfidf = vectorizer.fit_transform(X_train)\n","X_test_tfidf = vectorizer.transform(X_test)"]},{"cell_type":"markdown","metadata":{"id":"HzPzB8ud18_r"},"source":["## Shallow learning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4VhPszFI17uW"},"outputs":[],"source":["def classify_text(X_train, y_train, X_test, y_test, feature_name=\"features\"):\n","  # Logistic Regression\n","  lr = LogisticRegression(random_state=42).fit(X_train, y_train)\n","  y_pred = lr.predict(X_test)\n","  print(classification_report(y_test, y_pred, zero_division=0))\n","  print(f'The accuracy of the LogisticRegression using the {feature_name} is {round(accuracy_score(y_test, y_pred), 4)}')\n","\n","  # Random Forest\n","  rfc = RandomForestClassifier(max_depth=2, random_state=42).fit(X_train, y_train)\n","  y_pred = rfc.predict(X_test)\n","  print(classification_report(y_test, y_pred))\n","  print(f'The accuracy of the RandomForestClassifier using the {feature_name} is {round(accuracy_score(y_test, y_pred), 4)}')\n","\n","  # Decision Tree\n","  dtc = DecisionTreeClassifier().fit(X_train, y_train)\n","  y_pred = dtc.predict(X_test)\n","  print(classification_report(y_test, y_pred))\n","  print(f'The accuracy of the DecisionTreeClassifier using the {feature_name} is {round(accuracy_score(y_test, y_pred), 4)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1406149,"status":"ok","timestamp":1732313592432,"user":{"displayName":"Iker García Barranco","userId":"18047326103332880325"},"user_tz":-60},"id":"UagWa38P2YQd","outputId":"3b2ae1d2-9f3f-43fd-8d5f-ec7e6da029d9"},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","          gl       0.99      0.99      0.99     89757\n","          pt       1.00      1.00      1.00    210565\n","\n","    accuracy                           0.99    300322\n","   macro avg       0.99      0.99      0.99    300322\n","weighted avg       0.99      0.99      0.99    300322\n","\n","The accuracy of the LogisticRegression using the TF-IDF features is 0.995\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","          gl       0.30      1.00      0.46     89757\n","          pt       0.00      0.00      0.00    210565\n","\n","    accuracy                           0.30    300322\n","   macro avg       0.15      0.50      0.23    300322\n","weighted avg       0.09      0.30      0.14    300322\n","\n","The accuracy of the RandomForestClassifier using the TF-IDF features is 0.2989\n","              precision    recall  f1-score   support\n","\n","          gl       0.97      0.98      0.98     89757\n","          pt       0.99      0.99      0.99    210565\n","\n","    accuracy                           0.99    300322\n","   macro avg       0.98      0.98      0.98    300322\n","weighted avg       0.99      0.99      0.99    300322\n","\n","The accuracy of the DecisionTreeClassifier using the TF-IDF features is 0.9851\n"]}],"source":["# Usar la función con TF-IDF\n","classify_text(X_train_tfidf, y_train, X_test_tfidf, y_test, feature_name=\"TF-IDF features\")"]},{"cell_type":"markdown","metadata":{"id":"56DKdLhwx-LM"},"source":["## Clases necesarias"]},{"cell_type":"markdown","source":["Estas clases son utilizadas para manejar adecuadamente el vocabulario, vectorizer y dataset. Son las mismas que vimos en la práctica sobre CNN en clase."],"metadata":{"id":"St6pyeCWwFjw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"RQjiMEYRZ_Lx"},"outputs":[],"source":["class Vocabulary(object):\n","    \"\"\"Clase para procesar texto y extrar el vocabulario existente para su posterior mapeo.\"\"\"\n","\n","    def __init__(self, token_to_idx=None):\n","        \"\"\"\n","        Args:\n","            token_to_idx (dict): a pre-existing map of tokens to indices\n","        \"\"\"\n","\n","        if token_to_idx is None:\n","            token_to_idx = {}\n","        self._token_to_idx = token_to_idx\n","\n","        self._idx_to_token = {idx: token\n","                              for token, idx in self._token_to_idx.items()}\n","\n","    def to_serializable(self):\n","        \"\"\" returns a dictionary that can be serialized \"\"\"\n","        return {'token_to_idx': self._token_to_idx}\n","\n","    @classmethod\n","    def from_serializable(cls, contents):\n","        \"\"\" instantiates the Vocabulary from a serialized dictionary \"\"\"\n","        return cls(**contents)\n","\n","    def add_token(self, token):\n","        \"\"\"Update mapping dicts based on the token.\n","\n","        Args:\n","            token (str): the item to add into the Vocabulary\n","        Returns:\n","            index (int): the integer corresponding to the token\n","        \"\"\"\n","        if token in self._token_to_idx:\n","            index = self._token_to_idx[token]\n","        else:\n","            index = len(self._token_to_idx)\n","            self._token_to_idx[token] = index\n","            self._idx_to_token[index] = token\n","        return index\n","\n","    def add_many(self, tokens):\n","        \"\"\"Add a list of tokens into the Vocabulary\n","\n","        Args:\n","            tokens (list): a list of string tokens\n","        Returns:\n","            indices (list): a list of indices corresponding to the tokens\n","        \"\"\"\n","        return [self.add_token(token) for token in tokens]\n","\n","    def lookup_token(self, token):\n","        \"\"\"Retrieve the index associated with the token\n","\n","        Args:\n","            token (str): the token to look up\n","        Returns:\n","            index (int): the index corresponding to the token\n","        \"\"\"\n","\n","        return self._token_to_idx[token]\n","\n","    def lookup_index(self, index):\n","        \"\"\"Return the token associated with the index\n","\n","        Args:\n","            index (int): the index to look up\n","        Returns:\n","            token (str): the token corresponding to the index\n","        Raises:\n","            KeyError: if the index is not in the Vocabulary\n","        \"\"\"\n","        if index not in self._idx_to_token:\n","            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n","        return self._idx_to_token[index]\n","\n","    def __str__(self):\n","        return \"<Vocabulary(size=%d)>\" % len(self)\n","\n","    def __len__(self):\n","        return len(self._token_to_idx)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4rrc3lG9ADRe"},"outputs":[],"source":["class SequenceVocabulary(Vocabulary):\n","    \"\"\"Extensión de la clase vocabulario para lidiar con la secuencialidad del texto.\"\"\"\n","    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n","                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n","                 end_seq_token=\"<END>\"):\n","\n","        super(SequenceVocabulary, self).__init__(token_to_idx) #Llamamos al constructor padre.\n","\n","        self._mask_token = mask_token\n","        self._unk_token = unk_token\n","        self._begin_seq_token = begin_seq_token\n","        self._end_seq_token = end_seq_token\n","\n","        #Usamos el vocabulario inicial y añadimos los nuevos tokens de uso especial.\n","        self.mask_index = self.add_token(self._mask_token) #Padding\n","        self.unk_index = self.add_token(self._unk_token) #Tokens desconocidos\n","        self.begin_seq_index = self.add_token(self._begin_seq_token) #Inicio de una frase.\n","        self.end_seq_index = self.add_token(self._end_seq_token) #Fin de una frase.\n","\n","    def to_serializable(self):\n","        contents = super(SequenceVocabulary, self).to_serializable()\n","        contents.update({'unk_token': self._unk_token,\n","                         'mask_token': self._mask_token,\n","                         'begin_seq_token': self._begin_seq_token,\n","                         'end_seq_token': self._end_seq_token})\n","        return contents\n","\n","    def lookup_token(self, token):\n","        \"\"\"Retrieve the index associated with the token\n","          or the UNK index if token isn't present.\n","\n","        Args:\n","            token (str): the token to look up\n","        Returns:\n","            index (int): the index corresponding to the token\n","        Notes:\n","            `unk_index` needs to be >=0 (having been added into the Vocabulary)\n","              for the UNK functionality\n","        \"\"\"\n","        if self.unk_index >= 0:\n","            return self._token_to_idx.get(token, self.unk_index)\n","        else:\n","            return self._token_to_idx[token]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ADOavubAzVz"},"outputs":[],"source":["class LanguagesVectorizer(object):\n","    \"\"\" Usando el vocabulario, transforma cada token a su ID numérico y añade los tokens auxiliares necesarios (MASK, BEGIN, END...)\"\"\"\n","    def __init__(self, text_vocab, language_vocab):\n","        self.text_vocab = text_vocab #Vocabulario de todos los tokens que usamos.\n","        self.language_vocab = language_vocab #Vocabulario que representa las 4 categorias a las que estamos clasificando las noticias.\n","\n","    def vectorize(self, text, vector_length=-1):\n","        \"\"\"\n","        Args:\n","            text (str): the string of words separated by a space\n","            vector_length (int): an argument for forcing the length of index vector\n","        Returns:\n","            the vectorized text (numpy.array)\n","        \"\"\"\n","        indices = [self.text_vocab.begin_seq_index]\n","        indices.extend(self.text_vocab.lookup_token(token)\n","                       for token in text.split(\" \"))\n","        indices.append(self.text_vocab.end_seq_index)\n","\n","        if vector_length < 0:\n","            vector_length = len(indices)\n","\n","        #Todo 0s con longitud maxima\n","        out_vector = np.zeros(vector_length, dtype=np.int64)\n","        out_vector[:len(indices)] = indices\n","        out_vector[len(indices):] = self.text_vocab.mask_index\n","\n","        return out_vector\n","\n","    @classmethod\n","    def from_dataframe(cls, language_df, cutoff=5):\n","        \"\"\"Instantiate the vectorizer from the dataset dataframe\n","\n","        Args:\n","            language_df (pandas.DataFrame): the target dataset\n","            cutoff (int): frequency threshold for including in Vocabulary\n","        Returns:\n","            an instance of the LanguagesVectorizer\n","        \"\"\"\n","        language_vocab = Vocabulary()\n","        for language in sorted(set(language_df.language)):\n","            language_vocab.add_token(language)\n","\n","        word_counts = Counter()\n","        for text in language_df.text:\n","            for token in text.split(\" \"):\n","                if token not in string.punctuation:\n","                    word_counts[token] += 1\n","\n","        text_vocab = SequenceVocabulary()\n","        for word, word_count in word_counts.items():\n","            if word_count >= cutoff:\n","                text_vocab.add_token(word)\n","\n","        return cls(text_vocab, language_vocab)\n","\n","    @classmethod\n","    def from_serializable(cls, contents):\n","        text_vocab = \\\n","            SequenceVocabulary.from_serializable(contents['text_vocab'])\n","        language_vocab =  \\\n","            Vocabulary.from_serializable(contents['language_vocab'])\n","\n","        return cls(text_vocab=text_vocab, language_vocab=language_vocab)\n","\n","    def to_serializable(self):\n","        return {'text_vocab': self.text_vocab.to_serializable(),\n","                'language_vocab': self.language_vocab.to_serializable()}\n","\n","def generate_batches(dataset, batch_size, shuffle=True,\n","                     drop_last=True, device=\"cpu\"):\n","    \"\"\"\n","    A generator function which wraps the PyTorch DataLoader. It will\n","      ensure each tensor is on the write device location.\n","    \"\"\"\n","    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n","                            shuffle=shuffle, drop_last=drop_last)\n","\n","    for data_dict in dataloader:\n","        out_data_dict = {}\n","        for name, tensor in data_dict.items():\n","            out_data_dict[name] = data_dict[name].to(device)\n","        yield out_data_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5-TgEue_glGV"},"outputs":[],"source":["class LanguageDataset(Dataset):\n","    def __init__(self, language_df, vectorizer):\n","        \"\"\"\n","        Args:\n","            language_df (pandas.DataFrame): the dataset\n","            vectorizer (LanguagesVectorizer): vectorizer instatiated from dataset\n","        \"\"\"\n","        self.language_df = language_df\n","        self._vectorizer = vectorizer\n","\n","        # +1 if only using begin_seq, +2 if using both begin and end seq tokens\n","        measure_len = lambda context: len(context.split(\" \"))\n","        self._max_seq_length = max(map(measure_len, language_df.text)) + 2\n","\n","\n","        self.train_df = self.language_df[self.language_df.split=='train']\n","        self.train_size = len(self.train_df)\n","\n","        self.val_df = self.language_df[self.language_df.split=='val']\n","        self.validation_size = len(self.val_df)\n","\n","        self.test_df = self.language_df[self.language_df.split=='test']\n","        self.test_size = len(self.test_df)\n","\n","        self._lookup_dict = {'train': (self.train_df, self.train_size),\n","                             'val': (self.val_df, self.validation_size),\n","                             'test': (self.test_df, self.test_size)}\n","\n","        self.set_split('train')\n","\n","        # Class weights\n","        class_counts = language_df.language.value_counts().to_dict()\n","        def sort_key(item):\n","            return self._vectorizer.language_vocab.lookup_token(item[0])\n","        sorted_counts = sorted(class_counts.items(), key=sort_key)\n","        frequencies = [count for _, count in sorted_counts]\n","        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n","\n","\n","    @classmethod\n","    def load_dataset_and_make_vectorizer(cls, language_csv):\n","        \"\"\"Load dataset and make a new vectorizer from scratch\n","\n","        Args:\n","            surname_csv (str): location of the dataset\n","        Returns:\n","            an instance of SurnameDataset\n","        \"\"\"\n","        language_df = pd.read_csv(language_csv)\n","        train_language_df = language_df[language_df.split=='train']\n","        return cls(language_df, LanguagesVectorizer.from_dataframe(train_language_df))\n","\n","    @classmethod\n","    def load_dataset_and_load_vectorizer(cls, language_csv, vectorizer_filepath):\n","        \"\"\"Load dataset and the corresponding vectorizer.\n","        Used in the case in the vectorizer has been cached for re-use\n","\n","        Args:\n","            surname_csv (str): location of the dataset\n","            vectorizer_filepath (str): location of the saved vectorizer\n","        Returns:\n","            an instance of SurnameDataset\n","        \"\"\"\n","        language_df = pd.read_csv(language_csv)\n","        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n","        return cls(language_csv, vectorizer)\n","\n","    @staticmethod\n","    def load_vectorizer_only(vectorizer_filepath):\n","        \"\"\"a static method for loading the vectorizer from file\n","\n","        Args:\n","            vectorizer_filepath (str): the location of the serialized vectorizer\n","        Returns:\n","            an instance of LanguagesVectorizer\n","        \"\"\"\n","        with open(vectorizer_filepath) as fp:\n","            return LanguagesVectorizer.from_serializable(json.load(fp))\n","\n","    def save_vectorizer(self, vectorizer_filepath):\n","        \"\"\"saves the vectorizer to disk using json\n","\n","        Args:\n","            vectorizer_filepath (str): the location to save the vectorizer\n","        \"\"\"\n","        with open(vectorizer_filepath, \"w\") as fp:\n","            json.dump(self._vectorizer.to_serializable(), fp)\n","\n","    def get_vectorizer(self):\n","        \"\"\" returns the vectorizer \"\"\"\n","        return self._vectorizer\n","\n","    def set_split(self, split=\"train\"):\n","        \"\"\" selects the splits in the dataset using a column in the dataframe \"\"\"\n","        self._target_split = split\n","        self._target_df, self._target_size = self._lookup_dict[split]\n","\n","    def __len__(self):\n","        return self._target_size\n","\n","    def __getitem__(self, index):\n","        \"\"\"the primary entry point method for PyTorch datasets\n","\n","        Args:\n","            index (int): the index to the data point\n","        Returns:\n","            a dictionary holding the data point's features (x_data) and label (y_target)\n","        \"\"\"\n","        row = self._target_df.iloc[index]\n","\n","        text_value = str(row['text'])  # Forzar conversión a cadena\n","        text_vector = \\\n","            self._vectorizer.vectorize(text_value, self._max_seq_length)\n","\n","        language_index = \\\n","            self._vectorizer.language_vocab.lookup_token(row.language)\n","\n","        return {'x_data': text_vector,\n","                'y_target': language_index}\n","\n","    def get_num_batches(self, batch_size):\n","        \"\"\"Given a batch size, return the number of batches in the dataset\n","\n","        Args:\n","            batch_size (int)\n","        Returns:\n","            number of batches in the dataset\n","        \"\"\"\n","        return len(self) // batch_size\n","\n","def generate_batches(dataset, batch_size, shuffle=True,\n","                     drop_last=True, device=\"cpu\"):\n","    \"\"\"\n","    A generator function which wraps the PyTorch DataLoader. It will\n","      ensure each tensor is on the write device location.\n","    \"\"\"\n","    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n","                            shuffle=shuffle, drop_last=drop_last)\n","\n","    for data_dict in dataloader:\n","        out_data_dict = {}\n","        for name, tensor in data_dict.items():\n","            out_data_dict[name] = data_dict[name].to(device)\n","        yield out_data_dict"]},{"cell_type":"markdown","metadata":{"id":"p0Nshnr1sflP"},"source":["## Utils"]},{"cell_type":"markdown","metadata":{"id":"ItpZ3SKykULI"},"source":["Funciones para entrenar"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bl35Q06XwgvG"},"outputs":[],"source":["def make_train_state(args):\n","    return {'stop_early': False,\n","            'early_stopping_step': 0,\n","            'early_stopping_best_val': 1e8,\n","            'learning_rate': args.learning_rate,\n","            'epoch_index': 0,\n","            'train_loss': [],\n","            'train_acc': [],\n","            'val_loss': [],\n","            'val_acc': [],\n","            'test_loss': -1,\n","            'test_acc': -1,\n","            'model_filename': args.model_state_file}\n","\n","def update_train_state(args, model, train_state):\n","    \"\"\"Handle the training state updates.\n","\n","    Components:\n","     - Early Stopping: Prevent overfitting.\n","     - Model Checkpoint: Model is saved if the model is better\n","\n","    :param args: main arguments\n","    :param model: model to train\n","    :param train_state: a dictionary representing the training state values\n","    :returns:\n","        a new train_state\n","    \"\"\"\n","\n","    if train_state['epoch_index'] == 0:\n","        torch.save(model.state_dict(), train_state['model_filename'])\n","        train_state['stop_early'] = False\n","\n","    elif train_state['epoch_index'] >= 1:\n","        loss_tm1, loss_t = train_state['val_loss'][-2:]\n","\n","        if loss_t >= train_state['early_stopping_best_val']:\n","            # Update step\n","            train_state['early_stopping_step'] += 1\n","        else:\n","            if loss_t < train_state['early_stopping_best_val']:\n","                torch.save(model.state_dict(), train_state['model_filename'])\n","                train_state['early_stopping_best_val'] = loss_t\n","\n","\n","            train_state['early_stopping_step'] = 0\n","\n","        # Stop early ?\n","        train_state['stop_early'] = \\\n","            train_state['early_stopping_step'] >= args.early_stopping_criteria\n","\n","    return train_state\n","\n","def compute_accuracy(y_pred, y_target):\n","    _, y_pred_indices = y_pred.max(dim=1)\n","    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n","    return n_correct / len(y_pred_indices) * 100"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SeIy0gYKwgpm"},"outputs":[],"source":["def set_seed_everywhere(seed, cuda):\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if cuda:\n","        torch.cuda.manual_seed_all(seed)\n","\n","def download_embedding_file(language):\n","  fasttext.util.download_model(language, if_exists='ignore') # Descargar el modelo directamente\n","\n","def get_embedding_file(filename):\n","    model = fasttext.load_model(filename) # Cargamos el modelo descargado\n","    return model\n","\n","def make_embedding_matrix( fasttext_path_gl, fasttext_path_pt, words_idx_to_token, emb_dim, language):\n","  # Uso de un modelo u otro\n","  if language == \"gl\":\n","    ft = get_embedding_file(fasttext_path_gl)\n","  elif language == \"pt\":\n","    ft = get_embedding_file(fasttext_path_pt)\n","  final_embeddings = np.zeros((len(words_idx_to_token), emb_dim))\n","  for idx in sorted(words_idx_to_token):\n","      if ft.get_word_id(words_idx_to_token[idx]):\n","        # if the model does not have the word get_word_id returns -1, but fasttext models are able to make vectors even if a word is not in the vocabulary\n","        final_embeddings[idx, :] = ft.get_word_vector(words_idx_to_token[idx])\n","  # Cuando los embeddings ya estén borramos el modelo de fasttext de memoria\n","  del ft\n","  return final_embeddings"]},{"cell_type":"markdown","metadata":{"id":"IwmZf1vfCp5n"},"source":["## CNN y Perceptron"]},{"cell_type":"markdown","metadata":{"id":"rZrWxCW0mca4"},"source":["Clase para hacer un classifier con una CNN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6XsgUSrYma75"},"outputs":[],"source":["class CNN_NLP(nn.Module):\n","    \"\"\"An 1D Convulational Neural Network for Sentence Classification.\n","    Adapted from: https://www.kaggle.com/code/williamlwcliu/cnn-text-classification-pytorch\"\"\"\n","    def __init__(self,\n","                 pretrained_embedding=None,\n","                 freeze_embedding=False,\n","                 vocab_size=None,\n","                 embed_dim=100,\n","                 filter_sizes=[3, 4, 5],\n","                 num_filters=[100, 100, 100],\n","                 num_classes=4,\n","                 dropout=0.5):\n","        \"\"\"\n","        The constructor for CNN_NLP class.\n","\n","        Args:\n","            pretrained_embedding (torch.Tensor): Pretrained embeddings with\n","                shape (vocab_size, embed_dim)\n","            freeze_embedding (bool): Set to False to fine-tune pretraiend\n","                vectors. Default: False\n","            vocab_size (int): Need to be specified when not pretrained word\n","                embeddings are not used.\n","            embed_dim (int): Dimension of word vectors. Need to be specified\n","                when pretrained word embeddings are not used. Default: 100\n","            filter_sizes (List[int]): List of filter sizes. Default: [3, 4, 5]\n","            num_filters (List[int]): List of number of filters, has the same\n","                length as `filter_sizes`. Default: [100, 100, 100]\n","            n_classes (int): Number of classes. Default: 2\n","            dropout (float): Dropout rate. Default: 0.5\n","        \"\"\"\n","\n","        super(CNN_NLP, self).__init__()\n","        # Embedding layer\n","        if pretrained_embedding is not None:\n","            pretrained_embeddings = torch.from_numpy(pretrained_embedding).float()\n","            self.emb = nn.Embedding(embedding_dim=embed_dim,\n","                                    num_embeddings=vocab_size,\n","                                    padding_idx=0,\n","                                    _weight=pretrained_embeddings)\n","        else:\n","            self.embed_dim = embed_dim\n","            self.emb = nn.Embedding(num_embeddings=vocab_size,\n","                                          embedding_dim=embed_dim,\n","                                          padding_idx=0,\n","                                          max_norm=5.0)\n","        # Conv Network\n","        self.conv1d_list = nn.ModuleList([\n","            nn.Conv1d(in_channels=embed_dim,\n","                      out_channels=num_filters[i],\n","                      kernel_size=filter_sizes[i])\n","            for i in range(len(filter_sizes))\n","        ])\n","        # Fully-connected layer and Dropout\n","        self.fc = nn.Linear(np.sum(num_filters), num_classes)\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","    def forward(self, input_ids):\n","        \"\"\"Perform a forward pass through the network.\n","\n","        Args:\n","            input_ids (torch.Tensor): A tensor of token ids with shape\n","                (batch_size, max_sent_length)\n","\n","        Returns:\n","            logits (torch.Tensor): Output logits with shape (batch_size,\n","                n_classes)\n","        \"\"\"\n","\n","        # Get embeddings from `input_ids`. Output shape: (b, max_len, embed_dim)\n","        x_embed = self.emb(input_ids).float()\n","\n","        # Permute `x_embed` to match input shape requirement of `nn.Conv1d`.\n","        # Output shape: (b, embed_dim, max_len)\n","        x_reshaped = x_embed.permute(0, 2, 1)\n","\n","        # Apply CNN and ReLU. Output shape: (b, num_filters[i], L_out)\n","        x_conv_list = [F.relu(conv1d(x_reshaped)) for conv1d in self.conv1d_list]\n","\n","        # Max pooling. Output shape: (b, num_filters[i], 1)\n","        x_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2])\n","            for x_conv in x_conv_list]\n","\n","        # Concatenate x_pool_list to feed the fully connected layer.\n","        # Output shape: (b, sum(num_filters))\n","        x_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list],\n","                         dim=1)\n","\n","        # Compute logits. Output shape: (b, n_classes)\n","        logits = self.fc(self.dropout(x_fc))\n","\n","        return logits"]},{"cell_type":"markdown","metadata":{"id":"IowAth4AakmR"},"source":["Clase para hacer un classifier con un Perceptron de una fully connected layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"01IGFkbuJ4FD"},"outputs":[],"source":["class PerceptronClassifier(nn.Module):\n","    \"\"\"Perceptron based Classifier.\n","    \"\"\"\n","    def __init__(self, num_features):\n","        \"\"\"\n","        Args: num_features(init): The size of input feature vector.\n","        \"\"\"\n","        super(PerceptronClassifier, self).__init__()\n","        self.fc1 = nn.Linear(in_features=num_features,\n","                            out_features=1)\n","\n","    def forward(self, x_in, apply_sigmoid=False):\n","        \"\"\"The forward pass of the classifier\n","\n","        Args:\n","            x_in (torch.Tensor): an input data tensor\n","                x_in.shape should be (batch, num_features)\n","            apply_sigmoid (bool): a flag for the sigmoid activation\n","                should be false if used with the crossentropy losses.\n","        Returns:\n","            the resulting tensor. tensor.shape should be (batch,).\n","        \"\"\"\n","        y_out = self.fc1(x_in).squeeze()\n","        if apply_sigmoid:\n","            y_out = torch.sigmoid(y_out)\n","        return y_out"]},{"cell_type":"markdown","metadata":{"id":"BbrqxEYyEWo6"},"source":["## Creamos un DatasetDict con un subset de los datasets originales"]},{"cell_type":"markdown","metadata":{"id":"DulXNgRl3lkK"},"source":["Cogemos un subset de 1000 lineas por dataset y los juntamos"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40869,"status":"ok","timestamp":1736018286971,"user":{"displayName":"Iker García Barranco","userId":"18047326103332880325"},"user_tz":-60},"id":"J1-DlnnX1I_m","outputId":"e02a583d-16fe-4b23-a091-57390763e420"},"outputs":[{"output_type":"stream","name":"stdout","text":["Portugués: 1000 filas seleccionadas\n","Gallego: 1000 filas seleccionadas\n","Combinados: 2000 filas seleccionadas\n"]}],"source":["# Cargar ambos datasets\n","df_pt = pd.read_csv(file_path_pt)\n","df_gl = pd.read_csv(file_path_gl)\n","\n","# Seleccionar subconjuntos de cada dataset\n","def select_subset(df, train_size, test_size, val_size):\n","    train = df[df['split'] == 'train'].head(train_size)\n","    test = df[df['split'] == 'test'].head(test_size)\n","    val = df[df['split'] == 'val'].head(val_size)\n","    return pd.concat([train, test, val])\n","\n","# Seleccionar 700 de train, 150 de test, y 150 de val para cada idioma\n","df_pt_subset = select_subset(df_pt, train_size=700, test_size=150, val_size=150)\n","df_gl_subset = select_subset(df_gl, train_size=700, test_size=150, val_size=150)\n","\n","df_combined = pd.concat([df_pt_subset, df_gl_subset], ignore_index=True)\n","\n","# Verificar resultados\n","print(f\"Portugués: {len(df_pt_subset)} filas seleccionadas\")\n","print(f\"Gallego: {len(df_gl_subset)} filas seleccionadas\")\n","print(f\"Combinados: {len(df_combined)} filas seleccionadas\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1736018286971,"user":{"displayName":"Iker García Barranco","userId":"18047326103332880325"},"user_tz":-60},"id":"bo6W3tT4eEse","outputId":"d8427ced-3001-4c48-9d11-e177d19b2afe"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                text  split language\n","0  Deve a Ré ser condenada a pagar ao Autor a dif...  train       pt\n","1  O valor equivalente a 9/12 de uma remuneração,...  train       pt\n","2  O Acórdão recorrido está, salvo o devido respe...  train       pt\n","3  Há, por isso, erro de julgamento quando o Trib...  train       pt\n","4  Ora é pacificamente aceite que na tarefa de qu...  train       pt"],"text/html":["\n","  <div id=\"df-26180ac1-9fab-49d3-8e5b-43df2e55b173\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>split</th>\n","      <th>language</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Deve a Ré ser condenada a pagar ao Autor a dif...</td>\n","      <td>train</td>\n","      <td>pt</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>O valor equivalente a 9/12 de uma remuneração,...</td>\n","      <td>train</td>\n","      <td>pt</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>O Acórdão recorrido está, salvo o devido respe...</td>\n","      <td>train</td>\n","      <td>pt</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Há, por isso, erro de julgamento quando o Trib...</td>\n","      <td>train</td>\n","      <td>pt</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Ora é pacificamente aceite que na tarefa de qu...</td>\n","      <td>train</td>\n","      <td>pt</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-26180ac1-9fab-49d3-8e5b-43df2e55b173')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-26180ac1-9fab-49d3-8e5b-43df2e55b173 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-26180ac1-9fab-49d3-8e5b-43df2e55b173');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-fb89ff65-1974-4b73-9e07-a0d1c7ced92a\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fb89ff65-1974-4b73-9e07-a0d1c7ced92a')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-fb89ff65-1974-4b73-9e07-a0d1c7ced92a button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df_combined","summary":"{\n  \"name\": \"df_combined\",\n  \"rows\": 2000,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2000,\n        \"samples\": [\n          \"Martim jogral, que defeita,sempre convosco se deitavossa mulher! Vedes-me andar suspirando;e v\\u00f3s deitado, gozandovossa mulher! Do meu mal n\\u00e3o vos doeis;morro eu e v\\u00f3s fodeisvossa mulher! Jo\\u00e3o Garc\\u00eda de Guilhade (1) DE ESCARNIO E MALDICER Con chamarlle a este cap\\u00edtulo falcatruadas e maldades inocentes pod\\u00eda chegar. Pero este toque medieval perm\\u00edteme traer \\u00f3 trobador e a ilustraci\\u00f3n portugueses con m\\u00e1is naturalidade, \\\"y no tengo mas que decir\\\" sobre o tema, como di a\\u00ednda o outro de cando en vez. Estou dubidando se encetar polo principio, \\u00e9 dicir por orde cronol\\u00f3xica, ou se introducir o matiz diferenciador erudito entre escarnio e maldicer. O que poder\\u00eda pasar \\u00e9 que nalg\\u00fan caso as lindes entre un e outro fosen tan confusas, que non ser\\u00eda quen de tomar a decisi\\u00f3n m\\u00e1is axeitada. Poida que non exista a posibilidade obxectiva desa decisi\\u00f3n, e me vise obrigado a desandar o andado, adoito co rigor que sei se espera de min. Se ves que a concordancia co \\\"adoito\\\" non \\u00e9 a m\\u00e1is aquelada, ac\\u00e9ptanse suxesti\\u00f3ns. A escolla pod\\u00eda ser tam\\u00e9n entre \\\"pavadas\\\" e parvadas. Isto permitir\\u00eda unha distribuci\\u00f3n xeogr\\u00e1fica, sendo \\\"pavadas\\\" as bobadas, imbecilidades, necidades, sandeces e estupideces de orixe bonaerense, e parvadas as bobadas, imbecilidades, necidades, sandeces e estupideces referidas \\u00f3 resto do mundo. Claro que \\\"pavada\\\", que ven de pavo, tam\\u00e9n pode entenderse relacionada coa idade chamada do pavo xustamente, que se corresponde a ese per\\u00edodo rioplatense (por non reiterar o de bonaerense), e que coincide, ano arriba, ano abaixo, coa mi\\u00f1a estancia na Arxentina. Ben, imos al\\u00f3. Para min, como para o 99,9 por cen das xentes do teatro, a funci\\u00f3n \\u00e9 sagrada. Pero somos pobres seres humanos, e as veces cometemos sacrilexio. Nunha das representaci\\u00f3ns de \\\"El octavo d\\u00eda\\\", o meu primeiro traballo profesional hai case cincuenta anos, apareceu durante a actuaci\\u00f3n o fot\\u00f3grafo, co reportaxe que fixera, e coa emoci\\u00f3n de verme nas fotos esquec\\u00edn que ti\\u00f1a que entrar en escena. Os altofalantes daqueles interminables corredores dos camerinos do Teatro Alvear de Buenos Aires berraban o meu nome. Sa\\u00edn disparado, e entrei no escenario sen mirar, descubrindo que non era a mi\\u00f1a escena, xa que estaba s\\u00f3 Enrique Fava, o bispo. Axeonlleime para facer que rezaba e ga\\u00f1ar tempo ata situarme. Pouco a pouco, o resto de personaxes da escena mi\\u00f1a volveron a ocupar o seu sitio, xa que nos longos segundos que durou a espera desapareceran. O espect\\u00e1culo continuou, e a vergo\\u00f1a que pasei pola mi\\u00f1a irresponsabilidade serviume de lecci\\u00f3n para o futuro. Al\\u00e1 polo 1963, Jorge Petraglia, do que xa te\\u00f1o falado antes, decidiu repo\\u00f1er \\\"Esperando a Godot\\\", que estreara catro anos antes con gran repercusi\\u00f3n no ambiente teatral arxentino. Perdera o meu contacto (daquela eu non ti\\u00f1a nin m\\u00f3bil, nin correo electr\\u00f3nico, como \\u00e9 de imaxinar, nin sequera tel\\u00e9fono particular) e preguntou por min \\u00f3 \\\"amigo\\\", tam\\u00e9n actor, que deixara o posto en ARGENTORES que eu ocupei gracias \\u00e1 s\\u00faa recomendaci\\u00f3n. Petraglia quer\\u00eda que eu fixese o rapaz do texto de Becket. E o \\\"amigo\\\", posiblemente so\\u00f1ando con ese papel, mentiulle sobre o co\\u00f1ecemento de onde me pod\\u00eda atopar. Eu fun ver a funci\\u00f3n, e ben que sent\\u00edn e sentirei toda a vida non estar enriba daquel escenario. S\\u00f3 me queda o consolo de que o \\\"amigo\\\" tampouco estaba. Eu sei que \\\"fumar pode matar\\\" desde antes de que o puxesen nos paquetes de tabaco en letras grandes. Expl\\u00edcome. Era o ano 1986, nunha funci\\u00f3n de \\\"A noite vai coma un r\\u00edo\\\" de \\u00c1lvaro Cunqueiro co CDG. Mar\\u00eda Barcala, a Dona In\\u00e9s, preguntaba \\u00f3 Correo, que era eu, se hab\\u00eda carta. E o Correo respond\\u00edalle: \\\"Queiman as cartas os soldados, arrouban nas valixas por si ve\\u00f1en letras con noticias de tesouros. \\u00a1Ai, demo de guerra! O home non sabe da muller, o pai do fillo, non hai romer\\u00edas e a xente dorme no chan, co medo por almofada, e p\\u00e9rdese a ciencia de facer as camas. Os ducados veci\\u00f1os volv\\u00e9ronse tolos, andan os reis polos cami\\u00f1os, e nin se sementa pan....\\\" E a esas alturas do po\\u00e9tico texto de Cunqueiro descubro no chan de falso m\\u00e1rmore o meu paquete de \\\"Ducados\\\" que me caera da carteira do Correo. Naqueles segundos ata o final da escena, cando puiden recoller aquel obxecto, descubr\\u00edn, como xa dixen, que fumar pode matar. Se daquelas ata hoxe non deixei o vicio, \\u00e9 sinal de que o vicio ga\\u00f1oume a partida. Ningu\\u00e9n, nin sequera eu, \\u00e9 perfecto. En \\\"Cea para dous\\\", da que falarei m\\u00e1is adiante, traio pra esta secci\\u00f3n \\\"de escarnio e maldicer\\\" unha situaci\\u00f3n apropiada. Na publicidade deste espect\\u00e1culo figura Roberto Salgueiro como director, a\\u00ednda que non fose as\\u00ed. El estivo de axudante de direcci\\u00f3n meu, pero uns d\\u00edas antes da estrea deume o ultimatum de que ou figuraba el como director ou deixaba os ensaios. Ser\\u00eda o seu primeiro (e non lembro agora se \\u00faltimo) traballo profesional como director. \\\"O incerto se\\u00f1or don Hamlet\\\" (CDG, 1991) tam\\u00e9n deu materia. Ricard Salvat, director do espect\\u00e1culo, deixou para a eternidade a frase \\\"tu oc\\u00fapate de lo tuyo que de lo m\\u00edo me ocupo yo\\\", en resposta a un comentario da mi\\u00f1a amiga Susana Dans, que durante un descanso subiu \\u00f3 gali\\u00f1eiro do Teatro Principal de Santiago, onde ensaiabamos (no teatro, non no gali\\u00f1eiro), e informou a Salvat que dende al\\u00ed non se v\\u00eda todo o escenario. Era todo un personaxe o catal\\u00e1n. A min convidoume un d\\u00eda a comer (fac\\u00edao con m\\u00e1is xente do elenco nunha especie de campa\\u00f1a de ga\\u00f1ar adeptos), e cando me sento \\u00e1 mesa comenta que non sab\\u00eda por que me \\u00eda pagar el o men\\u00fa. A resposta l\\u00f3xica pola mi\\u00f1a parte ser\\u00eda abandonar o restaurante ou cambiar de mesa. Pero non. Sentei, com\\u00edn, beb\\u00edn, e \\u00f3 remate despedinme amablemente. Coido que foi o m\\u00e1is cruel castigo posible, obrigalo a pagar a s\\u00faa propia invitaci\\u00f3n. Eu son bastante mani\\u00e1tico coa puntualidade, e moito m\\u00e1is un d\\u00eda de funci\\u00f3n. Nunha representaci\\u00f3n de \\\"Xelmirez ou a gloria de Compostela\\\" (CDG, 1999) cheguei a tempo, pero moi xusto. O Director do CDG recibiume con acenos nervioso \\u00e1 entrada da Igrexa da Universidade onde se escenificaba a obra de Cortez\\u00f3n. En principio pensei que a cousa vi\\u00f1a de saber que eu sempre chego con moito adianto as funci\\u00f3ns, preocupado de que me tivese pasado algo. Pero que desencanto, non. Era que esa noite asist\\u00eda \\u00e1 representaci\\u00f3n o daquelas presidente da Xunta Manuel Fraga. Non hai moito, no outono do ano 2006, traballei en \\\"O regreso \\u00f3 deserto\\\", de Kolt\\u00e8s, baixo a direcci\\u00f3n de Cristina Dom\\u00ednguez. Nunha coreograf\\u00eda de transici\\u00f3n entre escenas un grupo de actores ti\\u00f1amos que sa\\u00edr dun gran caix\\u00f3n e dirixirnos en diversas direcci\\u00f3ns. Meu amigo Manolo Areoso sa\\u00eda da mi\\u00f1a dereita para irse cara a esquerda do escenario, e eu ti\\u00f1a que ir de fronte cara a boca da escena. El teimou, durante boa parte dos ensaios, en pasar por diante mi\\u00f1a, con risco de ter un accidente, sen facer caso \\u00e1s mi\\u00f1as recomendaci\\u00f3ns reiteradas un d\\u00eda si e outro tam\\u00e9n. Eu estes temas prefiro resolvelos \\u00e1 mi\\u00f1a maneira en lugar de recorrer \\u00e1 autoridade da directora ou da core\\u00f3grafa. Un d\\u00eda decid\\u00edn, logo dun minucioso c\\u00e1lculo matem\\u00e1tico, adiantar o p\\u00e9 dereito uns poucos cent\\u00edmetros. Tropezou co meu p\\u00e9, a\\u00ednda que non chegou a caer, pero o susto levouno, e desde aquela santo remedio. ((1)) Trobador galego-portugu\\u00e9s de Barcelos, s\\u00e9culo XII-XIII ^\",\n          \"Postos estes considerandos e sem os olvidarmos, decorre da pe\\u00e7a recursiva apresentada pela recorrente que pretende impugnar a mat\\u00e9ria de facto considerada como provada nos pontos 4, 7, 9, 10 e, 12 a 18, que em seu entender dever\\u00e3o ser julgados como n\\u00e3o provados ou apenas provado nos termos por si propostos.\",\n          \"Espa\\u00f1a sufrir\\u00e1 \\\"a tensi\\u00f3n da auga\\\"\\nA tensi\\u00f3n da auga quere dicir seca e desertificaci\\u00f3n. Significa que no futuro, \\u00e1 nosa lista de preocupaci\\u00f3ns, engadirase a falta de auga. Pero tam\\u00e9n o exceso: inundaci\\u00f3ns e crecidas de r\\u00edos. \\u00e9 a expresi\\u00f3n que utiliza a Axencia Europea de Medio Ambiente (EEA) para cualificar o que vai sufrir nas pr\\u00f3ximas d\\u00e9cadas Espa\\u00f1a e todos os pa\\u00edses da conca do Mediterr\\u00e1neo.A Uni\\u00f3n Europea publicou un informe chamado \\\"Impactos do cambio clim\\u00e1tico en Europa\\\" que especifica as convulsi\\u00f3ns que vivir\\u00e1 o vello continente polo quecemento global. O estudo analiza a cantidade da auga que o home necesitou entre 1975 e 2006 e, doutra banda, mostra a evidencia de que as choivas ser\\u00e1n moi intermitentes en Europa. Chover\\u00e1 pouco e necesitarase m\\u00e1is auga. \\\"No mesmo per\\u00edodo houbo un significativo incremento na demanda de auga en Espa\\u00f1a (entre o 50% e o 70%) e nas \\u00e1reas mediterr\\u00e1neas\\\", asevera o informe. E as predici\\u00f3ns de futuro van en li\\u00f1a ascendente. \\\"A demanda crecer\\u00e1 cada vez m\\u00e1is, especialmente no sur onde a necesidade de auga para a agricultura \\u00e9 maior. Con ela, desenvolverase unha competici\\u00f3n por este ben entre os distintos sectores (turismo, agricultura, enerx\\u00eda) e usos\\\".A\\u00ednda que o informe vaticina desertizaci\\u00f3n para Espa\\u00f1a, o estudo tampouco nos libra dos desbordamentos dos r\\u00edos. Haber\\u00e1 un incremento porque a alternancia entre per\\u00edodos de seca e precipitaci\\u00f3ns torrenciais fai a Espa\\u00f1a m\\u00e1is propensa a estas inundaci\\u00f3ns. Para 2080 prognostica que entre 2000 e 4000 persoas veranse afectadas as inundaci\\u00f3ns nas zonas costeiras pola subida do nivel do mar en Andaluc\\u00eda, Galicia, as Illas Baleares e Asturias. A rexi\\u00f3n m\\u00e1is afectada ser\\u00e1 o Pa\\u00eds Vasco: entre 4000 e 8000 persoas poder\\u00e1n ser v\\u00edtimas da subida do mar. Para mitigar todos estes problemas o informe aposta tanto pola reduci\\u00f3n do CO2 como pola adaptaci\\u00f3n \\u00e1s consecuencias do cambio que xa non se poden remediar. \\\"O 90% de todos os desastres que sucederon en Europa desde 1980 est\\u00e1n directa ou indirectamente relacionados co clima e representan o 95% das perdas econ\\u00f3micas causadas por cat\\u00e1strofes\\\", sinala o informe. Para evitar estas perdas ponse tres metas: a primeira, unha maior vixilancia, monitorizaci\\u00f3n e estudo dos cambios a nivel internacional; a segunda, estabilizar o clima para 2020 por baixo dos dous graos cent\\u00edgrados con respecto aos niveis pre industriais \\\"para evitar consecuencias irreversibles na sociedade e nos ecosistemas\\\". Por \\u00faltimo, fai fincap\\u00e9 na adaptaci\\u00f3n. Podes ver o informe completo no seguinte enlace: http://reports.eea.europa.eu/eea_report_2008_4/en/Fonte: El Pa\\u00eds\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"split\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"train\",\n          \"test\",\n          \"val\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"language\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"gl\",\n          \"pt\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":11}],"source":["df_combined.head()"]},{"cell_type":"code","source":["df_combined.to_csv('df_combined_sample.csv', index=False)"],"metadata":{"id":"JgJVr2pasVb5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HoaGFYZlB59N"},"source":["## CNN with galician fasttext embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YsGKHGAESnFb"},"outputs":[],"source":["args = Namespace(\n","    # Data and Path hyper parameters\n","    # languages_df= '/content/drive/My Drive/4º Carrera/NLP/Proyecto/df_combined_sample.csv',  # PATH: Luis\n","    languages_df= '/content/drive/MyDrive/4.Curso/NLP/Proyecto/df_combined_sample.csv',  # PATH: Iker\n","    # vectorizer_file= '/content/drive/My Drive/4º Carrera/NLP/Proyecto/vectorizer.json',  # PATH: Luis\n","    vectorizer_file=\"/content/drive/MyDrive/4.Curso/NLP/Proyecto/vectorizer.json\", # PATH: Iker\n","    model_state_file=\"model_storage/model_gl_embeddings.pth\",\n","    # Model hyper parameters\n","    use_emb=True,\n","    use_CNN=True,\n","    language='gl',\n","    # Training hyper parameter\n","    seed=42,\n","    learning_rate=0.001,\n","    dropout_p=0.1,\n","    batch_size=128,\n","    num_epochs=100,\n","    early_stopping_criteria=2,\n","    # Runtime option\n","    cuda=True,\n","    catch_keyboard_interrupt=True,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30199,"status":"ok","timestamp":1735925071894,"user":{"displayName":"Iker García Barranco","userId":"18047326103332880325"},"user_tz":-60},"id":"pBejOIgCbg3S","outputId":"de22f9fd-9b1f-428c-bb96-cc7d22963cd7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using CUDA: True\n","<Vocabulary(size=36952)>\n","Using pre-trained fasttext embeddings\n"]}],"source":["# Check CUDA\n","if not torch.cuda.is_available():\n","    args.cuda = False\n","\n","args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n","print(\"Using CUDA: {}\".format(args.cuda))\n","\n","# Set seed for reproducibility\n","set_seed_everywhere(args.seed, args.cuda)\n","\n","# Load dataset and vectorizer\n","dataset = LanguageDataset.load_dataset_and_make_vectorizer(args.languages_df)\n","dataset.save_vectorizer(args.vectorizer_file) # not necessary if we have a vectorizer already\n","vectorizer = dataset.get_vectorizer()\n","\n","if args.use_emb:\n","  text_vocab = vectorizer.text_vocab\n","  emb_gl_path = '/content/drive/MyDrive/4.Curso/NLP/Proyecto/embeddings/cc.gl.300.bin'# PATH: Iker\n","  # emb_gl_path = '/content/drive/MyDrive/4º Carrera/NLP/Proyecto/embeddings/cc.gl.300.bin'# PATH: Luis\n","  emb_pt_path = '/content/drive/MyDrive/4.Curso/NLP/Proyecto/embeddings/cc.pt.300.bin'# PATH: Iker\n","  # emb_pt_path = '/content/drive/MyDrive/4º Carrera/NLP/Proyecto/embeddings/cc.pt.300.bin'# PATH: Luis\n","\n","  embeddings = make_embedding_matrix(emb_gl_path, emb_pt_path, text_vocab._idx_to_token, 300, 'gl')\n","  print(\"Using pre-trained fasttext embeddings\")\n","else:\n","  print(\"Not using pre-trained embeddings\")\n","  embeddings = None\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":256,"status":"ok","timestamp":1735923941937,"user":{"displayName":"Iker García Barranco","userId":"18047326103332880325"},"user_tz":-60},"id":"BroR5dRwyxxb","outputId":"93ef52e2-b26a-440f-96fd-92a63ecd8983"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using CNN\n"]}],"source":["if args.use_CNN:\n","  classifier = CNN_NLP(pretrained_embedding=embeddings,\n","                        freeze_embedding=True,\n","                        vocab_size=len(vectorizer.text_vocab),  # Usa el número de filas de embeddings\n","                        embed_dim=embeddings.shape[1],   # Usa el número de columnas de embeddings\n","                        num_classes=len(vectorizer.language_vocab))\n","  print(\"Using CNN\")"]},{"cell_type":"markdown","metadata":{"id":"gqZmPclINyGR"},"source":["### Training"]},{"cell_type":"markdown","metadata":{"id":"uVO4svJXbfkT"},"source":["Training Loop"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1191214,"status":"ok","timestamp":1735926323908,"user":{"displayName":"Iker García Barranco","userId":"18047326103332880325"},"user_tz":-60},"id":"XHmji3CqjZUS","outputId":"eb696744-b066-4dee-d3c9-fc6c26cedba9"},"outputs":[{"output_type":"stream","name":"stdout","text":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","CNN_NLP                                  [128, 2]                  --\n","├─Embedding: 1-1                         [128, 10710, 300]         11,085,600\n","├─ModuleList: 1-2                        --                        --\n","│    └─Conv1d: 2-1                       [128, 100, 10708]         90,100\n","│    └─Conv1d: 2-2                       [128, 100, 10707]         120,100\n","│    └─Conv1d: 2-3                       [128, 100, 10706]         150,100\n","├─Dropout: 1-3                           [128, 300]                --\n","├─Linear: 1-4                            [128, 2]                  602\n","==========================================================================================\n","Total params: 11,446,502\n","Trainable params: 11,446,502\n","Non-trainable params: 0\n","Total mult-adds (G): 495.20\n","==========================================================================================\n","Input size (MB): 10.97\n","Forward/backward pass size (MB): 6579.30\n","Params size (MB): 45.79\n","Estimated Total Size (MB): 6636.06\n","==========================================================================================\n","In epoch 0 el vall_acc es 99.38038793103449 y el val_loss es 0.013770447666596235\n","Early stopping state is 0\n","In epoch 1 el vall_acc es 99.43426724137932 y el val_loss es 0.015760170248666265\n","Early stopping state is 0\n","In epoch 2 el vall_acc es 99.5959051724138 y el val_loss es 0.015300080377371697\n","Early stopping state is 0\n","In epoch 3 el vall_acc es 99.56896551724137 y el val_loss es 0.015483973614052581\n","Early stopping state is 1\n","In epoch 4 el vall_acc es 99.5689655172414 y el val_loss es 0.016550197562851122\n","Early stopping state is 2\n"]}],"source":["args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n","\n","classifier = classifier.to(args.device)\n","dataset.class_weights = dataset.class_weights.to(args.device)\n","\n","loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n","optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n","scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n","                                           mode='min', factor=0.5,\n","                                           patience=1)\n","\n","train_state = make_train_state(args)\n","\n","summary_shown = False\n","\n","try:\n","    for epoch_index in range(args.num_epochs):\n","        train_state['epoch_index'] = epoch_index\n","\n","        # Iterate over training dataset\n","\n","        # setup: batch generator, set loss and acc to 0, set train mode on\n","\n","        dataset.set_split('train')\n","        batch_generator = generate_batches(dataset,\n","                                           batch_size=args.batch_size,\n","                                           device=args.device)\n","        running_loss = 0.0\n","        running_acc = 0.0\n","        classifier.train()\n","\n","        for batch_index, batch_dict in enumerate(batch_generator):\n","            if not summary_shown:\n","                print(summary(classifier, input_data=[batch_dict['x_data']]))\n","                summary_shown = True\n","            # the training routine is these 5 steps:\n","\n","            # --------------------------------------\n","            # step 1. zero the gradients\n","            optimizer.zero_grad()\n","\n","            # step 2. compute the output\n","            y_pred = classifier(batch_dict['x_data'])\n","\n","            # step 3. compute the loss\n","            loss = loss_func(y_pred, batch_dict['y_target'])\n","            loss_t = loss.item()\n","            running_loss += (loss_t - running_loss) / (batch_index + 1)\n","\n","            # step 4. use loss to produce gradients\n","            loss.backward()\n","\n","            # step 5. use optimizer to take gradient step\n","            optimizer.step()\n","            # -----------------------------------------\n","            # compute the accuracy\n","            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n","            running_acc += (acc_t - running_acc) / (batch_index + 1)\n","\n","\n","        train_state['train_loss'].append(running_loss)\n","        train_state['train_acc'].append(running_acc)\n","\n","        # Iterate over val dataset\n","\n","        # setup: batch generator, set loss and acc to 0; set eval mode on\n","        dataset.set_split('val')\n","        batch_generator = generate_batches(dataset,\n","                                           batch_size=args.batch_size,\n","                                           device=args.device)\n","        running_loss = 0.\n","        running_acc = 0.\n","        classifier.eval()\n","\n","        for batch_index, batch_dict in enumerate(batch_generator):\n","\n","            # compute the output\n","            y_pred =  classifier(batch_dict['x_data'])\n","\n","            # step 3. compute the loss\n","            loss = loss_func(y_pred, batch_dict['y_target'])\n","            loss_t = loss.item()\n","            running_loss += (loss_t - running_loss) / (batch_index + 1)\n","\n","            # compute the accuracy\n","            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n","            running_acc += (acc_t - running_acc) / (batch_index + 1)\n","\n","\n","        train_state['val_loss'].append(running_loss)\n","        train_state['val_acc'].append(running_acc)\n","        print(f\"In epoch {epoch_index} el vall_acc es {running_acc} y el val_loss es {running_loss}\" )\n","\n","        train_state = update_train_state(args=args, model=classifier,\n","                                         train_state=train_state)\n","\n","        scheduler.step(train_state['val_loss'][-1])\n","        print(f\"Early stopping state is {train_state['early_stopping_step']}\")\n","        if train_state['stop_early']:\n","            break\n","\n","except KeyboardInterrupt:\n","    print(\"Exiting loop\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RO0y4KwUygWC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735926415796,"user_tz":-60,"elapsed":12095,"user":{"displayName":"Iker García Barranco","userId":"18047326103332880325"}},"outputId":"69d3686b-d3f5-44d0-dc73-d6bd4b6b74af"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-35-5ef9a1f24db8>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  classifier.load_state_dict(torch.load(train_state['model_filename']))\n"]},{"output_type":"stream","name":"stdout","text":["Test loss: 0.005528026166349923;\n","Test Accuracy: 99.83836206896552\n"]}],"source":["# compute the loss & accuracy on the test set using the best available model\n","\n","classifier.load_state_dict(torch.load(train_state['model_filename']))\n","\n","classifier = classifier.to(args.device)\n","dataset.class_weights = dataset.class_weights.to(args.device)\n","loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n","\n","dataset.set_split('test')\n","batch_generator = generate_batches(dataset,\n","                                   batch_size=args.batch_size,\n","                                   device=args.device)\n","running_loss = 0.\n","running_acc = 0.\n","classifier.eval()\n","\n","for batch_index, batch_dict in enumerate(batch_generator):\n","    # compute the output\n","    y_pred =  classifier(batch_dict['x_data'])\n","\n","    # compute the loss\n","    loss = loss_func(y_pred, batch_dict['y_target'])\n","    loss_t = loss.item()\n","    running_loss += (loss_t - running_loss) / (batch_index + 1)\n","\n","    # compute the accuracy\n","    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n","    running_acc += (acc_t - running_acc) / (batch_index + 1)\n","\n","train_state['test_loss'] = running_loss\n","train_state['test_acc'] = running_acc\n","print(\"Test loss: {};\".format(train_state['test_loss']))\n","print(\"Test Accuracy: {}\".format(train_state['test_acc']))"]},{"cell_type":"markdown","source":["## CNN with portuguese fasttext embeddings"],"metadata":{"id":"nQWkvI7ZR4Lp"}},{"cell_type":"code","source":["# Vaciar parte de la memoria de GPU\n","torch.cuda.empty_cache()"],"metadata":{"id":"aNBYs-PUUuBc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["args = Namespace(\n","    # Data and Path hyper parameters\n","    # languages_df= '/content/drive/My Drive/4º Carrera/NLP/Proyecto/df_combined_sample.csv',  # PATH: Luis\n","    languages_df= '/content/drive/MyDrive/4.Curso/NLP/Proyecto/df_combined_sample.csv',  # PATH: Iker\n","    # vectorizer_file= '/content/drive/My Drive/4º Carrera/NLP/Proyecto/vectorizer.json',  # PATH: Luis\n","    vectorizer_file=\"/content/drive/MyDrive/4.Curso/NLP/Proyecto/vectorizer.json\", # PATH: Iker\n","    model_state_file=\"model_storage/model_pt_embeddings.pth\",\n","    # Model hyper parameters\n","    use_emb=True,\n","    use_CNN=True,\n","    language='pt',\n","    # Training hyper parameter\n","    seed=42,\n","    learning_rate=0.001,\n","    dropout_p=0.1,\n","    batch_size=128,\n","    num_epochs=100,\n","    early_stopping_criteria=2,\n","    # Runtime option\n","    cuda=True,\n","    catch_keyboard_interrupt=True,\n",")"],"metadata":{"id":"HlG5CxEUR3sx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check CUDA\n","if not torch.cuda.is_available():\n","    args.cuda = False\n","\n","args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n","print(\"Using CUDA: {}\".format(args.cuda))\n","\n","# Set seed for reproducibility\n","set_seed_everywhere(args.seed, args.cuda)\n","\n","# Load dataset and vectorizer\n","dataset = LanguageDataset.load_dataset_and_make_vectorizer(args.languages_df)\n","dataset.save_vectorizer(args.vectorizer_file)\n","vectorizer = dataset.get_vectorizer()\n","\n","if args.use_emb:\n","  text_vocab = vectorizer.text_vocab\n","  print(text_vocab)\n","  # Si no los tenemos podemos descargar los archivos de embeddings de fasttext directamente con esta funcion\n","  # download_embedding_file('gl')\n","  # download_embedding_file('pt')\n","  emb_gl_path = '/content/drive/MyDrive/4.Curso/NLP/Proyecto/embeddings/cc.gl.300.bin'# PATH: Iker\n","  # emb_gl_path = '/content/drive/MyDrive/4º Carrera/NLP/Proyecto/embeddings/cc.gl.300.bin'# PATH: Luis\n","  emb_pt_path = '/content/drive/MyDrive/4.Curso/NLP/Proyecto/embeddings/cc.pt.300.bin'# PATH: Iker\n","  # emb_pt_path = '/content/drive/MyDrive/4º Carrera/NLP/Proyecto/embeddings/cc.pt.300.bin'# PATH: Luis\n","\n","  embeddings = make_embedding_matrix(emb_gl_path, emb_pt_path, text_vocab._idx_to_token, 300, 'pt')\n","  print(\"Using pre-trained fasttext embeddings\")\n","else:\n","  print(\"Not using pre-trained embeddings\")\n","  embeddings = None\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DxZGhNnVSgh5","outputId":"ad8cf418-2914-4a6f-c6a8-2890db45d8bd","executionInfo":{"status":"ok","timestamp":1736011101195,"user_tz":-60,"elapsed":44070,"user":{"displayName":"Iker García Barranco","userId":"18047326103332880325"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using CUDA: True\n","<Vocabulary(size=36952)>\n","Using pre-trained fasttext embeddings\n"]}]},{"cell_type":"code","source":["if args.use_CNN:\n","  classifier = CNN_NLP(pretrained_embedding=embeddings,\n","                        freeze_embedding=True, # Si congelamos los embeddings para que no se actualicen durante el training\n","                        vocab_size=len(vectorizer.text_vocab),  # Usa el número de filas de embeddings\n","                        embed_dim=embeddings.shape[1],   # Usa el número de columnas de embeddings\n","                        num_classes=len(vectorizer.language_vocab)) # 2, gallego y portugues\n","  print(\"Using CNN\")"],"metadata":{"id":"D1w8PHRSanlC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736011166247,"user_tz":-60,"elapsed":426,"user":{"displayName":"Iker García Barranco","userId":"18047326103332880325"}},"outputId":"249b1095-0db4-46a4-b631-135ee4e0f9c6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using CNN\n"]}]},{"cell_type":"markdown","metadata":{"id":"XDSuoVqba23v"},"source":["### Training"]},{"cell_type":"markdown","metadata":{"id":"55fNHwzDa23w"},"source":["Training Loop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UUdXproIa23w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736010367634,"user_tz":-60,"elapsed":1136945,"user":{"displayName":"Iker García Barranco","userId":"18047326103332880325"}},"outputId":"e3ea6aa4-22bd-4394-98ce-23f1923ba2d4"},"outputs":[{"output_type":"stream","name":"stdout","text":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","CNN_NLP                                  [128, 2]                  --\n","├─Embedding: 1-1                         [128, 10710, 300]         11,085,600\n","├─ModuleList: 1-2                        --                        --\n","│    └─Conv1d: 2-1                       [128, 100, 10708]         90,100\n","│    └─Conv1d: 2-2                       [128, 100, 10707]         120,100\n","│    └─Conv1d: 2-3                       [128, 100, 10706]         150,100\n","├─Dropout: 1-3                           [128, 300]                --\n","├─Linear: 1-4                            [128, 2]                  602\n","==========================================================================================\n","Total params: 11,446,502\n","Trainable params: 11,446,502\n","Non-trainable params: 0\n","Total mult-adds (G): 495.20\n","==========================================================================================\n","Input size (MB): 10.97\n","Forward/backward pass size (MB): 6579.30\n","Params size (MB): 45.79\n","Estimated Total Size (MB): 6636.06\n","==========================================================================================\n","In epoch 0 el vall_acc es 99.54202586206895 y el val_loss es 0.017295433995153372\n","Early stopping state is 0\n","In epoch 1 el vall_acc es 99.6228448275862 y el val_loss es 0.012998205288473903\n","Early stopping state is 0\n","In epoch 2 el vall_acc es 99.54202586206897 y el val_loss es 0.012911609133127432\n","Early stopping state is 0\n","In epoch 3 el vall_acc es 99.56896551724138 y el val_loss es 0.013183257078896435\n","Early stopping state is 1\n","In epoch 4 el vall_acc es 99.54202586206898 y el val_loss es 0.01391730586504403\n","Early stopping state is 2\n"]}],"source":["args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n","\n","classifier = classifier.to(args.device)\n","dataset.class_weights = dataset.class_weights.to(args.device)\n","\n","loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n","optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n","scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n","                                           mode='min', factor=0.5,\n","                                           patience=1)\n","\n","train_state = make_train_state(args)\n","\n","summary_shown = False\n","\n","try:\n","    for epoch_index in range(args.num_epochs):\n","        train_state['epoch_index'] = epoch_index\n","\n","        # Iterate over training dataset\n","\n","        # setup: batch generator, set loss and acc to 0, set train mode on\n","\n","        dataset.set_split('train')\n","        batch_generator = generate_batches(dataset,\n","                                           batch_size=args.batch_size,\n","                                           device=args.device)\n","        running_loss = 0.0\n","        running_acc = 0.0\n","        classifier.train()\n","\n","        for batch_index, batch_dict in enumerate(batch_generator):\n","            if not summary_shown:\n","                print(summary(classifier, input_data=[batch_dict['x_data']]))\n","                summary_shown = True\n","            # the training routine is these 5 steps:\n","\n","            # --------------------------------------\n","            # step 1. zero the gradients\n","            optimizer.zero_grad()\n","\n","            # step 2. compute the output\n","            y_pred = classifier(batch_dict['x_data'])\n","\n","            # step 3. compute the loss\n","            loss = loss_func(y_pred, batch_dict['y_target'])\n","            loss_t = loss.item()\n","            running_loss += (loss_t - running_loss) / (batch_index + 1)\n","\n","            # step 4. use loss to produce gradients\n","            loss.backward()\n","\n","            # step 5. use optimizer to take gradient step\n","            optimizer.step()\n","            # -----------------------------------------\n","            # compute the accuracy\n","            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n","            running_acc += (acc_t - running_acc) / (batch_index + 1)\n","\n","\n","        train_state['train_loss'].append(running_loss)\n","        train_state['train_acc'].append(running_acc)\n","\n","        # Iterate over val dataset\n","\n","        # setup: batch generator, set loss and acc to 0; set eval mode on\n","        dataset.set_split('val')\n","        batch_generator = generate_batches(dataset,\n","                                           batch_size=args.batch_size,\n","                                           device=args.device)\n","        running_loss = 0.\n","        running_acc = 0.\n","        classifier.eval()\n","\n","        for batch_index, batch_dict in enumerate(batch_generator):\n","\n","            # compute the output\n","            y_pred =  classifier(batch_dict['x_data'])\n","\n","            # step 3. compute the loss\n","            loss = loss_func(y_pred, batch_dict['y_target'])\n","            loss_t = loss.item()\n","            running_loss += (loss_t - running_loss) / (batch_index + 1)\n","\n","            # compute the accuracy\n","            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n","            running_acc += (acc_t - running_acc) / (batch_index + 1)\n","\n","\n","        train_state['val_loss'].append(running_loss)\n","        train_state['val_acc'].append(running_acc)\n","        print(f\"In epoch {epoch_index} el vall_acc es {running_acc} y el val_loss es {running_loss}\" )\n","\n","        train_state = update_train_state(args=args, model=classifier,\n","                                         train_state=train_state)\n","\n","        scheduler.step(train_state['val_loss'][-1])\n","        print(f\"Early stopping state is {train_state['early_stopping_step']}\")\n","        if train_state['stop_early']:\n","            break\n","\n","except KeyboardInterrupt:\n","    print(\"Exiting loop\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GLch7lR_a23w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736010379635,"user_tz":-60,"elapsed":12008,"user":{"displayName":"Iker García Barranco","userId":"18047326103332880325"}},"outputId":"00eaed83-a35c-4ac6-a0d3-14f15dc23b82"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-22-5ef9a1f24db8>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  classifier.load_state_dict(torch.load(train_state['model_filename']))\n"]},{"output_type":"stream","name":"stdout","text":["Test loss: 0.008437231710935331;\n","Test Accuracy: 99.78448275862068\n"]}],"source":["# compute the loss & accuracy on the test set using the best available model\n","\n","classifier.load_state_dict(torch.load(train_state['model_filename']))\n","\n","classifier = classifier.to(args.device)\n","dataset.class_weights = dataset.class_weights.to(args.device)\n","loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n","\n","dataset.set_split('test')\n","batch_generator = generate_batches(dataset,\n","                                   batch_size=args.batch_size,\n","                                   device=args.device)\n","running_loss = 0.\n","running_acc = 0.\n","classifier.eval()\n","\n","for batch_index, batch_dict in enumerate(batch_generator):\n","    # compute the output\n","    y_pred =  classifier(batch_dict['x_data'])\n","\n","    # compute the loss\n","    loss = loss_func(y_pred, batch_dict['y_target'])\n","    loss_t = loss.item()\n","    running_loss += (loss_t - running_loss) / (batch_index + 1)\n","\n","    # compute the accuracy\n","    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n","    running_acc += (acc_t - running_acc) / (batch_index + 1)\n","\n","train_state['test_loss'] = running_loss\n","train_state['test_acc'] = running_acc\n","print(\"Test loss: {};\".format(train_state['test_loss']))\n","print(\"Test Accuracy: {}\".format(train_state['test_acc']))"]},{"cell_type":"markdown","source":["## Perceptron"],"metadata":{"id":"TcuQzJUSQK50"}},{"cell_type":"markdown","source":["### Clases para funcionamiento de Perceptron"],"metadata":{"id":"PHWOtdKLmE1-"}},{"cell_type":"markdown","source":["Estas clases son utilizadas para manejar adecuadamente el vocabulario, vectorizer y dataset. Son las mismas que vimos en la práctica sobre Perceptron en clase."],"metadata":{"id":"8T_-yrUHwX7I"}},{"cell_type":"code","source":["class PerceptronVocabulary(object):\n","    \"\"\"Clase para procesar texto y extrar el vocabulario existente para su posterior mapeo.\n","    \"\"\"\n","    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"\"):\n","        \"\"\"\n","        Args: token_to_idx(dict): Pre existing map of Tokens to Index.\n","            : add_unk(bool): A flag indicating whether to add UNK Token.\n","            : unk_token(string): The UNK Token to add in Vocabulary.\n","        \"\"\"\n","        if token_to_idx is None:\n","            token_to_idx = {}\n","        self._token_to_idx = token_to_idx\n","        self._idx_to_token = {idx:token for token,idx in self._token_to_idx.items()}\n","        self._add_unk = add_unk\n","        self._unk_token = unk_token\n","        self.unk_index = -1\n","        if add_unk:\n","            self.unk_index = self.add_token(unk_token)\n","\n","    def add_token(self, token):\n","        \"\"\"Update the mapping dictionary based on the Tokens.\n","        Args: token: The item to add into the Vocabulary.\n","        Returns: index: Integer corresponding to the Token.\n","        \"\"\"\n","        if token in self._token_to_idx:\n","            index = self._token_to_idx[token]\n","        else:\n","            index = len(self._token_to_idx)\n","            self._token_to_idx[token] = index\n","            self._idx_to_token[index] = token\n","        return index\n","\n","    def add_many(self, tokens):\n","        \"\"\"Add a list of Tokens into Vocabulary.\n","        Args: tokens(list): A list of string Tokens.\n","        Returns: indices(list): A list of indices correspoinding to the Tokens.\n","        \"\"\"\n","        return [self.add_token(token) for token in tokens]\n","\n","    def lookup_token(self, token):\n","        \"\"\"Retrieve the Index associated with the Token.\n","        Args: token(str): The Token to lookup.\n","        Returns: index(int): The Index correspoinding to the Token.\n","        \"\"\"\n","        if self.unk_index >= 0:\n","            return self._token_to_idx.get(token, self.unk_index)\n","        else:\n","            return self._token_to_idx[token]\n","\n","    def lookup_index(self, index):\n","        \"\"\"Return the Token associated with the Index.\n","        Args: index(int): The Index to lookup.\n","        Returns: token(str): The Token correspoinding to the Index.\n","        \"\"\"\n","        if index not in self._idx_to_token:\n","            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n","        return self._idx_to_token[index]\n","\n","    def __str__(self):\n","        return \"<Vocabulary (size=%d)\" % len(self)\n","\n","    def __len__(self):\n","        return len(self._token_to_idx)"],"metadata":{"id":"REl6gkdEl6V3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PerceptronVectorizer(object):\n","    \"\"\"The Vectorizer coordinates the Vocabularies and puts them to use.\n","    \"\"\"\n","    def __init__(self, text_vocab, language_vocab):\n","        self.text_vocab = text_vocab #Vocabulario de todos los tokens que usamos.\n","        self.language_vocab = language_vocab #Vocabulario que representa las 4 categorias a las que estamos clasificando las noticias.\n","\n","    def vectorize(self, review):\n","        \"\"\"Create a collasped Onehot Vector for the review.\n","        Args: review: The review\n","        Returns: one_hot: The collapsed one hot Encoding.\n","        \"\"\"\n","        one_hot = np.zeros(len(self.text_vocab), dtype=np.float32)\n","        for token in review.split(\" \"):\n","            if token not in string.punctuation:\n","                one_hot[self.text_vocab.lookup_token(token)] = 1\n","        return one_hot\n","\n","    @classmethod\n","    def from_dataframe(cls, language_df, cutoff=5):\n","        \"\"\"Instantiate the vectorizer from the dataset dataframe\n","\n","        Args:\n","            language_df (pandas.DataFrame): the target dataset\n","            cutoff (int): frequency threshold for including in Vocabulary\n","        Returns:\n","            an instance of the LanguagesVectorizer\n","        \"\"\"\n","        language_vocab = PerceptronVocabulary()\n","        for language in sorted(set(language_df.language)):\n","            language_vocab.add_token(language)\n","\n","        word_counts = Counter()\n","        for text in language_df.text:\n","            for token in text.split(\" \"):\n","                if token not in string.punctuation:\n","                    word_counts[token] += 1\n","\n","        text_vocab = PerceptronVocabulary(add_unk=True)\n","        for word, word_count in word_counts.items():\n","            if word_count >= cutoff:\n","                text_vocab.add_token(word)\n","\n","        return cls(text_vocab, language_vocab)\n","\n","    def get_text_vocab(self):\n","        \"\"\"Returns the review Vocabulary.\n","        \"\"\"\n","        return self.text_vocab"],"metadata":{"id":"_S8rPUDchVqd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PerceptronDataset(Dataset):\n","    def __init__(self, language_df, vectorizer):\n","        \"\"\"\n","        Args: language_df(pandas.DataFrame): The dataset.\n","            : vectorizer(ReviewVectorizer): Vector instantiated from dataset.\n","        \"\"\"\n","        self.language_df = language_df\n","        self._vectorizer = vectorizer\n","\n","        self.train_df = self.language_df[self.language_df.split == \"train\"]\n","        self.train_size = len(self.train_df)\n","\n","        self.val_df = self.language_df[self.language_df.split == \"val\"]\n","        self.validation_size = len(self.val_df)\n","\n","        self.test_df = self.language_df[self.language_df.split == \"test\"]\n","        self.test_size = len(self.test_df)\n","\n","        self._lookup_dict = {\"train\": (self.train_df, self.train_size),\n","                                \"val\": (self.val_df, self.validation_size),\n","                                \"test\": (self.test_df, self.test_size)}\n","        print({\"train\": self.train_size,\n","                                \"val\": self.validation_size,\n","                                \"test\": self.test_size})\n","        self.set_split(\"train\")\n","\n","    @classmethod\n","    def load_dataset_and_make_vectorizer(cls, language_csv):\n","        \"\"\"Load dataset and make new vectorizer from scratch.\n","        Args: language_csv: Location of the dataset.\n","        Returns: An instance of ReviewDataset.\n","        \"\"\"\n","        language_df = pd.read_csv(language_csv)\n","        train_language_df = language_df[language_df.split == \"train\"]\n","        return cls(language_df, PerceptronVectorizer.from_dataframe(train_language_df))\n","\n","    def save_vectorizer(self, vectorizer_filepath):\n","        with open(vectorizer_filepath, \"w\") as fp:\n","            json.dump(self._vectorizer.to_serializable(), fp)\n","\n","    def get_vectorizer(self):\n","        \"\"\"Returns the Vectorizer.\n","        \"\"\"\n","        return self._vectorizer\n","\n","    def set_split(self, split=\"train\"):\n","        \"\"\"Splits the dataset using a column in the DataFrame.\n","        Args: split(str): One of \"train\", \"val\" or \"test\"\n","        \"\"\"\n","        self._target_split = split\n","        self._target_df, self._target_size = self._lookup_dict[split]\n","\n","    def __len__(self):\n","        return self._target_size\n","\n","    def __getitem__(self, index):\n","        \"\"\"Primary entry point of PyTorch Datasets.\n","        Args: index: Index of the Datapoint.\n","        Returns: A dictionary holding the Data point features and labels.\n","        \"\"\"\n","\n","        row = self._target_df.iloc[index]\n","        text_value = str(row['text'])  # Forzar conversión a cadena\n","        text_vector = self._vectorizer.vectorize(text_value)\n","        language_index = self._vectorizer.language_vocab.lookup_token(row.language)\n","        return {\"x_data\": text_vector,\n","                \"y_target\": torch.tensor(language_index, dtype=torch.float32)}\n","\n","    def get_num_batches(self, batch_size):\n","        \"\"\"Given a batch size, return the number of batches in the Dataset.\n","        Args: batch_size(int)\n","        Returns: Number of batches in the Dataset.\n","        \"\"\"\n","        return len(self) // batch_size"],"metadata":{"id":"Em0AMX2UjIlW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_accuracy(y_pred, y_target):\n","    y_pred_indices = (torch.sigmoid(y_pred)>0.5).long()\n","    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n","    return n_correct / len(y_pred_indices) * 100"],"metadata":{"id":"K5gF-5RwoiTu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f9syUOM-RSs9"},"source":["### Training"]},{"cell_type":"code","source":["# Vaciar parte de la memoria de GPU\n","torch.cuda.empty_cache()"],"metadata":{"id":"jnHh01tcUBHE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["args = Namespace(\n","    # Data and Path hyper parameters\n","    # languages_df= '/content/drive/My Drive/4º Carrera/NLP/Proyecto/df_combined_sample.csv',  # PATH: Luis\n","    languages_df= '/content/drive/MyDrive/4.Curso/NLP/Proyecto/df_combined_sample.csv',  # PATH: Iker\n","    # vectorizer_file= '/content/drive/My Drive/4º Carrera/NLP/Proyecto/vectorizer.json',  # PATH: Luis\n","    vectorizer_file=\"/content/drive/MyDrive/4.Curso/NLP/Proyecto/vectorizer.json\", # PATH: Iker\n","    model_state_file=\"model_storage/perceptron.pth\",\n","    # Training hyper parameter\n","    seed=42,\n","    learning_rate=0.001,\n","    batch_size=128,\n","    num_epochs=50,\n","    # Runtime option\n","    cuda=True,\n",")"],"metadata":{"id":"jbHfwp6ZQZDt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kjM76dYoRSs9"},"source":["Training Loop"]},{"cell_type":"code","source":["def generate_batches(dataset, batch_size, shuffle=True,\n","                     drop_last=True, device=\"cpu\"):\n","  \"\"\"A generator function which wraps the PyTorch DataLoader.\n","  \"\"\"\n","  dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n","                          shuffle=shuffle, drop_last=drop_last)\n","  for data_dict in dataloader:\n","    out_data_dict = {}\n","    for name, tensor in data_dict.items():\n","      out_data_dict[name] = data_dict[name].to(device)\n","    yield out_data_dict"],"metadata":{"id":"WrdyOuC8xQXC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check CUDA\n","if not torch.cuda.is_available():\n","    args.cuda = False\n","\n","args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n","print(\"Using CUDA: {}\".format(args.cuda))\n","\n","# Set seed for reproducibility\n","set_seed_everywhere(args.seed, args.cuda)\n","\n","# Load dataset and vectorizer\n","dataset = PerceptronDataset.load_dataset_and_make_vectorizer(args.languages_df)\n","vectorizer = dataset.get_vectorizer()\n","classifier = PerceptronClassifier(num_features=len(vectorizer.text_vocab))\n","\n","loss_func = nn.BCEWithLogitsLoss()\n","optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n","\n","train_state = make_train_state(args)\n","\n","#Empezamos el outer loop que itera N veces siendo N el número de epochs.\n","for epoch_index in range(args.num_epochs):\n","    train_state['epoch_index'] = epoch_index\n","\n","    # Generamos los training batches con el train set.\n","    dataset.set_split('train')\n","    batch_generator = generate_batches(dataset, batch_size=args.batch_size, device=args.device)\n","    running_loss = 0.0\n","    running_acc = 0.0\n","    #Llamamos al método .train() del clasificador para indicar que el modelo está en \"modo de entrenamiento\" y los parámetros del modelo son mutables.\n","    classifier.train()\n","    for batch_index, batch_dict in enumerate(batch_generator):\n","        # Por cada minibatch ejecutamos la rutina de entrenamiento:\n","        # step 1. Ponemos a 0 los gradientes\n","        optimizer.zero_grad()\n","        # step 2. Calculamos el ouput del clasificador en este paso.\n","        y_pred = classifier(x_in=batch_dict['x_data'].float())\n","        # step 3. Calculamos el loss del clasificador en este paso.\n","        loss = loss_func(y_pred, batch_dict['y_target'].float())\n","        loss_batch = loss.item()\n","        running_loss += (loss_batch - running_loss) / (batch_index + 1)\n","        # step 4. Usamos el loss para producir los gradientes.\n","        loss.backward()\n","        # step 5.Usamos  el optimizador para dar un paso de gradiente\n","        optimizer.step()\n","        # --------------------------------------------------\n","        #Calculmos el accuracy para tener una métrica extra además del loss.\n","        acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n","        running_acc += (acc_batch - running_acc) / (batch_index + 1)\n","    print(\"Train loss: {:.3f} in Epoch {}\".format(running_loss, epoch_index))\n","    print(\"Train accuracy: {:.3f} in Epoch {}\".format(running_acc, epoch_index))\n","    train_state['train_loss'].append(running_loss)\n","    train_state['train_acc'].append(running_acc)\n","    # Iterate over val dataset\n","\n","    # setup: batch generator, set loss and acc to 0, set eval mode on\n","    dataset.set_split('val')\n","    batch_generator = generate_batches(dataset, batch_size=args.batch_size, device=args.device)\n","    running_loss = 0.\n","    running_acc = 0.\n","    classifier.eval()\n","    for batch_index, batch_dict in enumerate(batch_generator):\n","        # step 1. compute the output\n","        y_pred = classifier(x_in=batch_dict['x_data'].float())\n","        # step 2. compute the loss\n","        loss = loss_func(y_pred, batch_dict['y_target'].float())\n","        loss_batch = loss.item()\n","        running_loss += (loss_batch - running_loss) / (batch_index + 1)\n","        # step 3. compute the accuracy\n","        acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n","        running_acc += (acc_batch - running_acc) / (batch_index + 1)\n","    print(\"Val loss: {:.3f} in Epoch {}\".format(running_loss, epoch_index))\n","    print(\"Val accuracy: {:.3f} in Epoch {}\".format(running_acc, epoch_index))\n","    train_state['val_loss'].append(running_loss)\n","    train_state['val_acc'].append(running_acc)\n","\n","\n","torch.save(classifier.state_dict(), train_state['model_filename'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8kWN3EEIt6Y-","executionInfo":{"status":"ok","timestamp":1736019416336,"user_tz":-60,"elapsed":422907,"user":{"displayName":"Iker García Barranco","userId":"18047326103332880325"}},"outputId":"de1e43e9-5669-4f19-ff0d-bbacc895dcab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using CUDA: False\n","{'train': 17500, 'val': 3750, 'test': 3750}\n","Train loss: -0.605 in Epoch 0\n","Train accuracy: 49.822 in Epoch 0\n","Val loss: -1.522 in Epoch 0\n","Val accuracy: 50.081 in Epoch 0\n","Train loss: -2.303 in Epoch 1\n","Train accuracy: 50.017 in Epoch 1\n","Val loss: -3.066 in Epoch 1\n","Val accuracy: 50.000 in Epoch 1\n","Train loss: -3.868 in Epoch 2\n","Train accuracy: 50.023 in Epoch 2\n","Val loss: -4.641 in Epoch 2\n","Val accuracy: 50.000 in Epoch 2\n","Train loss: -5.489 in Epoch 3\n","Train accuracy: 49.989 in Epoch 3\n","Val loss: -6.288 in Epoch 3\n","Val accuracy: 49.838 in Epoch 3\n","Train loss: -7.143 in Epoch 4\n","Train accuracy: 49.983 in Epoch 4\n","Val loss: -7.928 in Epoch 4\n","Val accuracy: 50.000 in Epoch 4\n","Train loss: -8.823 in Epoch 5\n","Train accuracy: 49.989 in Epoch 5\n","Val loss: -9.623 in Epoch 5\n","Val accuracy: 49.865 in Epoch 5\n","Train loss: -10.522 in Epoch 6\n","Train accuracy: 50.006 in Epoch 6\n","Val loss: -11.298 in Epoch 6\n","Val accuracy: 49.973 in Epoch 6\n","Train loss: -12.237 in Epoch 7\n","Train accuracy: 50.006 in Epoch 7\n","Val loss: -13.058 in Epoch 7\n","Val accuracy: 49.811 in Epoch 7\n","Train loss: -13.969 in Epoch 8\n","Train accuracy: 49.977 in Epoch 8\n","Val loss: -14.741 in Epoch 8\n","Val accuracy: 49.892 in Epoch 8\n","Train loss: -15.696 in Epoch 9\n","Train accuracy: 50.006 in Epoch 9\n","Val loss: -16.394 in Epoch 9\n","Val accuracy: 50.135 in Epoch 9\n","Train loss: -17.436 in Epoch 10\n","Train accuracy: 49.983 in Epoch 10\n","Val loss: -18.119 in Epoch 10\n","Val accuracy: 50.081 in Epoch 10\n","Train loss: -19.168 in Epoch 11\n","Train accuracy: 50.029 in Epoch 11\n","Val loss: -19.910 in Epoch 11\n","Val accuracy: 49.919 in Epoch 11\n","Train loss: -20.920 in Epoch 12\n","Train accuracy: 50.040 in Epoch 12\n","Val loss: -21.733 in Epoch 12\n","Val accuracy: 49.758 in Epoch 12\n","Train loss: -22.658 in Epoch 13\n","Train accuracy: 50.034 in Epoch 13\n","Val loss: -23.313 in Epoch 13\n","Val accuracy: 50.054 in Epoch 13\n","Train loss: -24.444 in Epoch 14\n","Train accuracy: 49.971 in Epoch 14\n","Val loss: -25.146 in Epoch 14\n","Val accuracy: 49.946 in Epoch 14\n","Train loss: -26.191 in Epoch 15\n","Train accuracy: 49.994 in Epoch 15\n","Val loss: -26.873 in Epoch 15\n","Val accuracy: 49.973 in Epoch 15\n","Train loss: -27.940 in Epoch 16\n","Train accuracy: 50.040 in Epoch 16\n","Val loss: -28.524 in Epoch 16\n","Val accuracy: 50.135 in Epoch 16\n","Train loss: -29.706 in Epoch 17\n","Train accuracy: 50.029 in Epoch 17\n","Val loss: -30.312 in Epoch 17\n","Val accuracy: 50.027 in Epoch 17\n","Train loss: -31.491 in Epoch 18\n","Train accuracy: 49.989 in Epoch 18\n","Val loss: -32.079 in Epoch 18\n","Val accuracy: 50.054 in Epoch 18\n","Train loss: -33.250 in Epoch 19\n","Train accuracy: 50.029 in Epoch 19\n","Val loss: -33.814 in Epoch 19\n","Val accuracy: 50.054 in Epoch 19\n","Train loss: -35.020 in Epoch 20\n","Train accuracy: 50.023 in Epoch 20\n","Val loss: -35.609 in Epoch 20\n","Val accuracy: 49.973 in Epoch 20\n","Train loss: -36.769 in Epoch 21\n","Train accuracy: 50.017 in Epoch 21\n","Val loss: -37.198 in Epoch 21\n","Val accuracy: 50.189 in Epoch 21\n","Train loss: -38.561 in Epoch 22\n","Train accuracy: 50.006 in Epoch 22\n","Val loss: -39.073 in Epoch 22\n","Val accuracy: 50.027 in Epoch 22\n","Train loss: -40.279 in Epoch 23\n","Train accuracy: 50.040 in Epoch 23\n","Val loss: -40.940 in Epoch 23\n","Val accuracy: 49.919 in Epoch 23\n","Train loss: -42.083 in Epoch 24\n","Train accuracy: 50.011 in Epoch 24\n","Val loss: -42.531 in Epoch 24\n","Val accuracy: 50.081 in Epoch 24\n","Train loss: -43.870 in Epoch 25\n","Train accuracy: 49.994 in Epoch 25\n","Val loss: -44.425 in Epoch 25\n","Val accuracy: 50.000 in Epoch 25\n","Train loss: -45.656 in Epoch 26\n","Train accuracy: 49.971 in Epoch 26\n","Val loss: -46.166 in Epoch 26\n","Val accuracy: 49.946 in Epoch 26\n","Train loss: -47.425 in Epoch 27\n","Train accuracy: 49.971 in Epoch 27\n","Val loss: -47.748 in Epoch 27\n","Val accuracy: 50.135 in Epoch 27\n","Train loss: -49.197 in Epoch 28\n","Train accuracy: 50.000 in Epoch 28\n","Val loss: -49.764 in Epoch 28\n","Val accuracy: 49.919 in Epoch 28\n","Train loss: -50.926 in Epoch 29\n","Train accuracy: 50.011 in Epoch 29\n","Val loss: -51.397 in Epoch 29\n","Val accuracy: 50.000 in Epoch 29\n","Train loss: -52.715 in Epoch 30\n","Train accuracy: 50.006 in Epoch 30\n","Val loss: -53.050 in Epoch 30\n","Val accuracy: 50.027 in Epoch 30\n","Train loss: -54.490 in Epoch 31\n","Train accuracy: 50.011 in Epoch 31\n","Val loss: -55.015 in Epoch 31\n","Val accuracy: 49.919 in Epoch 31\n","Train loss: -56.263 in Epoch 32\n","Train accuracy: 50.011 in Epoch 32\n","Val loss: -56.741 in Epoch 32\n","Val accuracy: 49.946 in Epoch 32\n","Train loss: -58.018 in Epoch 33\n","Train accuracy: 50.011 in Epoch 33\n","Val loss: -58.458 in Epoch 33\n","Val accuracy: 49.973 in Epoch 33\n","Train loss: -59.860 in Epoch 34\n","Train accuracy: 49.971 in Epoch 34\n","Val loss: -60.076 in Epoch 34\n","Val accuracy: 50.135 in Epoch 34\n","Train loss: -61.585 in Epoch 35\n","Train accuracy: 49.994 in Epoch 35\n","Val loss: -61.942 in Epoch 35\n","Val accuracy: 49.973 in Epoch 35\n","Train loss: -63.362 in Epoch 36\n","Train accuracy: 49.994 in Epoch 36\n","Val loss: -63.722 in Epoch 36\n","Val accuracy: 50.000 in Epoch 36\n","Train loss: -65.105 in Epoch 37\n","Train accuracy: 50.023 in Epoch 37\n","Val loss: -65.500 in Epoch 37\n","Val accuracy: 49.973 in Epoch 37\n","Train loss: -66.935 in Epoch 38\n","Train accuracy: 49.994 in Epoch 38\n","Val loss: -67.097 in Epoch 38\n","Val accuracy: 50.027 in Epoch 38\n","Train loss: -68.637 in Epoch 39\n","Train accuracy: 50.046 in Epoch 39\n","Val loss: -68.772 in Epoch 39\n","Val accuracy: 50.108 in Epoch 39\n","Train loss: -70.476 in Epoch 40\n","Train accuracy: 49.983 in Epoch 40\n","Val loss: -70.519 in Epoch 40\n","Val accuracy: 50.081 in Epoch 40\n","Train loss: -72.252 in Epoch 41\n","Train accuracy: 49.989 in Epoch 41\n","Val loss: -72.542 in Epoch 41\n","Val accuracy: 49.946 in Epoch 41\n","Train loss: -74.050 in Epoch 42\n","Train accuracy: 49.989 in Epoch 42\n","Val loss: -74.177 in Epoch 42\n","Val accuracy: 50.054 in Epoch 42\n","Train loss: -75.835 in Epoch 43\n","Train accuracy: 49.960 in Epoch 43\n","Val loss: -76.221 in Epoch 43\n","Val accuracy: 49.838 in Epoch 43\n","Train loss: -77.521 in Epoch 44\n","Train accuracy: 50.017 in Epoch 44\n","Val loss: -77.906 in Epoch 44\n","Val accuracy: 49.946 in Epoch 44\n","Train loss: -79.356 in Epoch 45\n","Train accuracy: 49.989 in Epoch 45\n","Val loss: -79.604 in Epoch 45\n","Val accuracy: 49.973 in Epoch 45\n","Train loss: -81.089 in Epoch 46\n","Train accuracy: 50.017 in Epoch 46\n","Val loss: -81.342 in Epoch 46\n","Val accuracy: 49.946 in Epoch 46\n","Train loss: -82.893 in Epoch 47\n","Train accuracy: 49.983 in Epoch 47\n","Val loss: -83.138 in Epoch 47\n","Val accuracy: 50.000 in Epoch 47\n","Train loss: -84.633 in Epoch 48\n","Train accuracy: 50.011 in Epoch 48\n","Val loss: -84.728 in Epoch 48\n","Val accuracy: 50.027 in Epoch 48\n","Train loss: -86.334 in Epoch 49\n","Train accuracy: 50.040 in Epoch 49\n","Val loss: -86.414 in Epoch 49\n","Val accuracy: 50.054 in Epoch 49\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736019623605,"user_tz":-60,"elapsed":1849,"user":{"displayName":"Iker García Barranco","userId":"18047326103332880325"}},"outputId":"50ca7e8d-7c7f-4b65-965d-740ec01ce14b","id":"acHiUBfWRSs-"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-26-5aa054e4da56>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  classifier.load_state_dict(torch.load(train_state['model_filename']))\n"]},{"output_type":"stream","name":"stdout","text":["Test loss: 125523.11314655174;\n","Test Accuracy: 50.13469827586207\n"]}],"source":["# compute the loss & accuracy on the test set using the best available model\n","classifier.load_state_dict(torch.load(train_state['model_filename']))\n","\n","classifier = classifier.to(args.device)\n","loss_func = nn.CrossEntropyLoss()\n","\n","dataset.set_split('test')\n","batch_generator = generate_batches(dataset,\n","                                   batch_size=args.batch_size,\n","                                   device=args.device)\n","running_loss = 0.\n","running_acc = 0.\n","classifier.eval()\n","\n","for batch_index, batch_dict in enumerate(batch_generator):\n","    # compute the output\n","    y_pred =  classifier(batch_dict['x_data'])\n","\n","    # compute the loss\n","    loss = loss_func(y_pred, batch_dict['y_target'])\n","    loss_t = loss.item()\n","    running_loss += (loss_t - running_loss) / (batch_index + 1)\n","\n","    # compute the accuracy\n","    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n","    running_acc += (acc_t - running_acc) / (batch_index + 1)\n","\n","train_state['test_loss'] = running_loss\n","train_state['test_acc'] = running_acc\n","print(\"Test loss: {};\".format(train_state['test_loss']))\n","print(\"Test Accuracy: {}\".format(train_state['test_acc']))"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["56DKdLhwx-LM","p0Nshnr1sflP","IwmZf1vfCp5n","BbrqxEYyEWo6","HoaGFYZlB59N","nQWkvI7ZR4Lp","XDSuoVqba23v","PHWOtdKLmE1-"],"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}